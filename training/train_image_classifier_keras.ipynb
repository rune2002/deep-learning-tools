{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Flatten, Dense, Activation, Lambda, Cropping2D, Dropout\n",
    "from keras.layers import Convolution2D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lmdb(lmdb_data):\n",
    "    lmdb_env = lmdb.open(lmdb_data)\n",
    "    lmdb_txn = lmdb_env.begin()\n",
    "    lmdb_cursor = lmdb_txn.cursor()\n",
    "    datum = caffe_pb2.Datum()\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for key, value in lmdb_cursor:\n",
    "        datum.ParseFromString(value)\n",
    "        label = datum.label\n",
    "        data = caffe.io.datum_to_array(datum)\n",
    "    \n",
    "        #CxHxW to HxWxC in cv2\n",
    "        image = np.transpose(data, (1,2,0))\n",
    "        X.append(image)\n",
    "        y.append(label)\n",
    "        \n",
    "    lmdb_env.close()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lmdb data\n",
    "\n",
    "data_path = \"../lmdb/\"\n",
    "train_file = data_path + \"train_lmdb/\"\n",
    "valid_file= data_path + \"val_lmdb/\"\n",
    "test_file = data_path + \"test_lmdb/\"\n",
    "\n",
    "X_train, y_train = load_lmdb(train_file)\n",
    "X_valid, y_valid = load_lmdb(valid_file)\n",
    "X_test, y_test = load_lmdb(test_file)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(X):\n",
    "    # Convert to grayscale\n",
    "    n = np.shape(X)[0]\n",
    "    gray = []\n",
    "    for i in range(0, n):\n",
    "        gray.append(cv2.cvtColor(X[i], cv2.COLOR_RGB2GRAY))\n",
    "        \n",
    "    # Normalize the data\n",
    "    normal = []\n",
    "    for i in range(0, n):\n",
    "        one_channel = np.zeros((32, 32, 1), np.float)\n",
    "        one_channel[:,:,0] = ((gray[i].astype(np.float) - 128) / 128) \n",
    "        normal.append(one_channel)\n",
    "        \n",
    "    return normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(preprocess_image(X_train))\n",
    "X_valid = np.array(preprocess_image(X_valid))\n",
    "X_test = np.array(preprocess_image(X_test))\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your architecture here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6003, 32, 32, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Define your architecture here.\n",
    "model = Sequential()\n",
    "# model.add(Convolution2D(6,5,5,subsample=(1,1),activation=\"relu\",input_shape=(32,32,1)))\n",
    "model.add(Conv2D(6, (5, 5), activation=\"relu\", input_shape=(32, 32, 1), strides=(1, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "# model.add(Convolution2D(16,5,5,subsample=(1,1),activation=\"relu\"))\n",
    "model.add(Conv2D(16, (5, 5), activation=\"relu\", strides=(1, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(84))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "Before each epoch, shuffle the training set.\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "Save the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 6003 samples, validate on 2062 samples\n",
      "Epoch 1/20\n",
      "6003/6003 [==============================] - 4s 633us/step - loss: 1.3775 - val_loss: 0.4818\n",
      "Epoch 2/20\n",
      "6003/6003 [==============================] - 4s 693us/step - loss: 0.4394 - val_loss: 0.2603\n",
      "Epoch 3/20\n",
      "6003/6003 [==============================] - 5s 881us/step - loss: 0.2703 - val_loss: 0.1917\n",
      "Epoch 4/20\n",
      "6003/6003 [==============================] - 4s 692us/step - loss: 0.1787 - val_loss: 0.1533\n",
      "Epoch 5/20\n",
      "6003/6003 [==============================] - 4s 668us/step - loss: 0.1393 - val_loss: 0.1249\n",
      "Epoch 6/20\n",
      "6003/6003 [==============================] - 4s 678us/step - loss: 0.1075 - val_loss: 0.1199\n",
      "Epoch 7/20\n",
      "6003/6003 [==============================] - 5s 769us/step - loss: 0.0830 - val_loss: 0.1172\n",
      "Epoch 8/20\n",
      "6003/6003 [==============================] - 4s 738us/step - loss: 0.0746 - val_loss: 0.0991\n",
      "Epoch 9/20\n",
      "6003/6003 [==============================] - 4s 687us/step - loss: 0.0628 - val_loss: 0.0993\n",
      "Epoch 10/20\n",
      "6003/6003 [==============================] - 5s 792us/step - loss: 0.0492 - val_loss: 0.1000\n",
      "Epoch 11/20\n",
      "6003/6003 [==============================] - 4s 737us/step - loss: 0.0402 - val_loss: 0.1034\n",
      "Epoch 12/20\n",
      "6003/6003 [==============================] - 4s 733us/step - loss: 0.0380 - val_loss: 0.1114\n",
      "Epoch 13/20\n",
      "6003/6003 [==============================] - 4s 625us/step - loss: 0.0363 - val_loss: 0.1018\n",
      "Epoch 14/20\n",
      "6003/6003 [==============================] - 4s 672us/step - loss: 0.0308 - val_loss: 0.1004\n",
      "Epoch 15/20\n",
      "6003/6003 [==============================] - 4s 615us/step - loss: 0.0239 - val_loss: 0.0994\n",
      "Epoch 16/20\n",
      "6003/6003 [==============================] - 4s 617us/step - loss: 0.0197 - val_loss: 0.1067\n",
      "Epoch 17/20\n",
      "6003/6003 [==============================] - 4s 628us/step - loss: 0.0191 - val_loss: 0.0917\n",
      "Epoch 18/20\n",
      "6003/6003 [==============================] - 4s 610us/step - loss: 0.0193 - val_loss: 0.0991\n",
      "Epoch 19/20\n",
      "6003/6003 [==============================] - 4s 617us/step - loss: 0.0144 - val_loss: 0.0960\n",
      "Epoch 20/20\n",
      "6003/6003 [==============================] - 4s 624us/step - loss: 0.0100 - val_loss: 0.0990\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history=model.fit(X_train, y_train, batch_size=128, epochs=20, validation_data=(X_valid, y_valid))\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "Be sure to only do this once!\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "1935/1935 [==============================] - 0s 237us/step\n",
      "test loss, test acc: 0.08067338933827979\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(X_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
